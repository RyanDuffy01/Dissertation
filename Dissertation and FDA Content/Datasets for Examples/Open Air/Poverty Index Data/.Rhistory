model_2_predicted <- predict(model_2,newdata=test_data_rf)
model_2_predicted <- predict(model_2,newdata=test_data_rf)
model_2_residuals <- split_rf_data$test$price - model_2_predicted
MSE_model_2 <- sum((split_rf_data$test$price-mean_price)^2)
SSE_model_2 <- sum(model_2_residuals^2)
model_2_RMSE <- sqrt((1/length(model_2_residuals))*SSE_model_2)
model_2_R_Square <- 1-SSE_model_2/MSE_model_2
# Fits regression tree model with all coefficients
model_3 <- rpart(
formula = price ~ .,
data    = train_data,
method  = "anova"
)
model_3_predicted <- predict(model_3,newdata=test_data)
model_3_residuals <- test_data$price - model_3_predicted
MSE_model_3 <- sum((test_data$price-mean_price)^2)
SSE_model_3 <- sum(model_3_residuals^2)
model_3_RMSE <- sqrt((1/length(model_3_residuals))*SSE_model_3)
model_3_R_Square <- 1-SSE_model_3/MSE_model_3
data.frame(
Model=paste("Model",1:3),
RMSE=c(model_1_RMSE,model_2_RMSE,model_3_RMSE),
`R Squared`= c(model_1_R_Square,model_2_R_Square,model_3_R_Square)
)
# finds the prices model 1 predicts for the cars in the testing dataset
model_1_predicted <- predict(model_1,newdata = split_data$test)
# find the difference between the predicted and actual prices of the cars in the testing dataset
model_1_residuals <- split_data$test$price - model_1_predicted
# finds the sum of squared differences between prices of cars and the mean
ME_model_1 <- sum((split_data$test$price-mean_price)^2)
# finds the sum of squared differences between the predicted and actual prices of cars
SSE_model_1 <- sum(model_1_residuals^2)
# finds the root mean squared error of the model based in the testing dataset
model_1_RMSE <- sqrt((1/length(model_1_residuals))*SSE_model_1)
# finds the R Squared Value of the testing dataset
model_1_R_Square <- 1-SSE_model_1/ME_model_1
model_2_predicted <- predict(model_2,newdata=test_data_rf)
model_3_predicted <- predict(model_3,newdata=test_data)
ME_model_3 <- sum((test_data$price-mean_price)^2)
SSE_model_3 <- sum(model_3_residuals^2)
model_3_RMSE <- sqrt((1/length(model_3_residuals))*SSE_model_3)
data.frame(
Model=paste("Model",1:3),
RMSE=c(model_1_RMSE,model_2_RMSE,model_3_RMSE),
`R Squared`= c(model_1_R_Square,model_2_R_Square,model_3_R_Square)
)
varImpPlot(model_2)
varImpPlot(model_2)
varImpPlot(model_2,title="Random Forets Model")
varImpPlot(model_2,title="Random Forest Model")
varImpPlot(model_2,main="Random Forest Model")
varImpPlot(model_2,main="Random Forest Model")
citation(fda)
citation("fda")
library(fda)
library(ggplot2)
library(tidyverse)
knitr::opts_chunk$set(echo = FALSE)
basis_mortality_rates <- create.bspline.basis(rangeval=range(Years),breaks=Years,norder=4)
library(fda)
library(ggplot2)
library(tidyverse)
knitr::opts_chunk$set(echo = FALSE)
set.seed(190005680)
function_ex_1 <- function(Time){
(1/2)*(Time+Time^2+exp(-1/5*(Time))+exp(-(1/30)*Time+8))
}
sigma_2_errors <- 30
Time_Grid <- seq(0,40,length.out=40)
Observations <- function_ex_1(Time_Grid)
Number_Of_Observations <- length(Observations)
Observation_Errors <- rnorm(Number_Of_Observations,0,sigma_2_errors)
ex_1_data <- data.frame(
Time = Time_Grid,
Y=Observations+Observation_Errors
)
ggplot(ex_1_data,aes(x=Time,y=Y))+
geom_point()+
geom_line(data=data.frame(Time=Time_Grid,Y=Observations),col="red")
mortality_rates_wide <- read_csv("C:/Users/ryand/Documents/Dissertation/Dissertation and FDA Content/Datasets for Examples/Alcohol Example/Alcohol Mortality Rates Per Country.csv", skip = 6)
mortality_rates_long <- pivot_longer(mortality_rates_wide,values_to="MortalityRate",cols=2:5,names_to = "Country")
ggplot(mortality_rates_long,aes(x=Year,y=MortalityRate,col=Country))+
geom_point()+
ylab("Age-standardised death rates per 100,000 people")+
xlim(c(2000,2022))
Years <- unique(mortality_rates_long$Year)
Scot_UnE <- read.csv("C:/Users/ryand/Documents/Dissertation/Dissertation and FDA Content/Datasets for Examples/Alcohol Example/Scotland Unemployment.csv",skip=7) %>%
rename("Year"=Important.notes,"Unemployment_Rate"=X) %>%
filter(Year %in% Years) %>%
mutate(Year=as.numeric(Year)) %>%
mutate(Country="Scotland")
Eng_UnE <- read.csv("C:/Users/ryand/Documents/Dissertation/Dissertation and FDA Content/Datasets for Examples/Alcohol Example/England Unemployment.csv",skip=7) %>%
rename("Year"=Important.notes,"Unemployment_Rate"=X) %>%
filter(Year %in% Years) %>%
mutate(Year=as.numeric(Year)) %>%
mutate(Country="England")
Wales_UnE <- read.csv("C:/Users/ryand/Documents/Dissertation/Dissertation and FDA Content/Datasets for Examples/Alcohol Example/Wales Unemployment.csv",skip=7) %>%
rename("Year"=Important.notes,"Unemployment_Rate"=X) %>%
filter(Year %in% Years) %>%
mutate(Year=as.numeric(Year)) %>%
mutate(Country="Wales")
NI_UnE <- read.csv("C:/Users/ryand/Documents/Dissertation/Dissertation and FDA Content/Datasets for Examples/Alcohol Example/Northern Ireland Unemployment.csv",skip=7) %>%
rename("Year"=Important.notes,"Unemployment_Rate"=X) %>%
filter(Year %in% Years) %>%
mutate(Year=as.numeric(Year)) %>%
mutate(Country="Northern Ireland")
All_Countries_UnE <- bind_rows(Scot_UnE,Eng_UnE,Wales_UnE,NI_UnE)
All_Countries_UnE_Wide <- pivot_wider(All_Countries_UnE,names_from = Country,values_from = Unemployment_Rate)
ggplot(All_Countries_UnE,aes(x=Year,y=Unemployment_Rate,col=Country))+
geom_point()+
ylab("Unemployment Rate (%)") +
xlim(c(2000,2022))
basis_mortality_rates <- create.bspline.basis(rangeval=range(Years),breaks=Years,norder=4)
library(fda)
library(ggplot2)
library(tidyverse)
knitr::opts_chunk$set(echo = FALSE)
Years <- unique(mortality_rates_long$Year)
mortality_rates_wide <- read_csv("C:/Users/ryand/Documents/Dissertation/Dissertation and FDA Content/Datasets for Examples/Alcohol Example/Alcohol Mortality Rates Per Country.csv", skip = 6)
mortality_rates_wide <- read_csv("C:/Users/ryand/Documents/Dissertation/Dissertation and FDA Content/Datasets for Examples/Alcohol Example/Alcohol Mortality Rates Per Country.csv", skip = 6)
mortality_rates_long <- pivot_longer(mortality_rates_wide,values_to="MortalityRate",cols=2:5,names_to = "Country")
mortality_rates_long <- pivot_longer(mortality_rates_wide,values_to="MortalityRate",cols=2:5,names_to = "Country")
ggplot(mortality_rates_long,aes(x=Year,y=MortalityRate,col=Country))+
geom_point()+
ylab("Age-standardised death rates per 100,000 people")+
xlim(c(2000,2022))
Years <- unique(mortality_rates_long$Year)
length(Years)
library(fda)
library(ggplot2)
library(tidyverse)
knitr::opts_chunk$set(echo = FALSE)
observation_matrix <- data.matrix(mortality_rates_wide %>% dplyr::select(-c(Country)))
library(fda)
library(ggplot2)
library(tidyverse)
knitr::opts_chunk$set(echo = FALSE)
set.seed(190005680)
function_ex_1 <- function(Time){
(1/2)*(Time+Time^2+exp(-1/5*(Time))+exp(-(1/30)*Time+8))
}
sigma_2_errors <- 30
Time_Grid <- seq(0,40,length.out=40)
Observations <- function_ex_1(Time_Grid)
Number_Of_Observations <- length(Observations)
Observation_Errors <- rnorm(Number_Of_Observations,0,sigma_2_errors)
ex_1_data <- data.frame(
Time = Time_Grid,
Y=Observations+Observation_Errors
)
ggplot(ex_1_data,aes(x=Time,y=Y))+
geom_point()+
geom_line(data=data.frame(Time=Time_Grid,Y=Observations),col="red")
mortality_rates_wide <- read_csv("C:/Users/ryand/Documents/Dissertation/Dissertation and FDA Content/Datasets for Examples/Alcohol Example/Alcohol Mortality Rates Per Country.csv", skip = 6)
mortality_rates_long <- pivot_longer(mortality_rates_wide,values_to="MortalityRate",cols=2:5,names_to = "Country")
ggplot(mortality_rates_long,aes(x=Year,y=MortalityRate,col=Country))+
geom_point()+
ylab("Age-standardised death rates per 100,000 people")+
xlim(c(2000,2022))
Years <- unique(mortality_rates_long$Year)
Scot_UnE <- read.csv("C:/Users/ryand/Documents/Dissertation/Dissertation and FDA Content/Datasets for Examples/Alcohol Example/Scotland Unemployment.csv",skip=7) %>%
rename("Year"=Important.notes,"Unemployment_Rate"=X) %>%
filter(Year %in% Years) %>%
mutate(Year=as.numeric(Year)) %>%
mutate(Country="Scotland")
Eng_UnE <- read.csv("C:/Users/ryand/Documents/Dissertation/Dissertation and FDA Content/Datasets for Examples/Alcohol Example/England Unemployment.csv",skip=7) %>%
rename("Year"=Important.notes,"Unemployment_Rate"=X) %>%
filter(Year %in% Years) %>%
mutate(Year=as.numeric(Year)) %>%
mutate(Country="England")
Wales_UnE <- read.csv("C:/Users/ryand/Documents/Dissertation/Dissertation and FDA Content/Datasets for Examples/Alcohol Example/Wales Unemployment.csv",skip=7) %>%
rename("Year"=Important.notes,"Unemployment_Rate"=X) %>%
filter(Year %in% Years) %>%
mutate(Year=as.numeric(Year)) %>%
mutate(Country="Wales")
NI_UnE <- read.csv("C:/Users/ryand/Documents/Dissertation/Dissertation and FDA Content/Datasets for Examples/Alcohol Example/Northern Ireland Unemployment.csv",skip=7) %>%
rename("Year"=Important.notes,"Unemployment_Rate"=X) %>%
filter(Year %in% Years) %>%
mutate(Year=as.numeric(Year)) %>%
mutate(Country="Northern Ireland")
All_Countries_UnE <- bind_rows(Scot_UnE,Eng_UnE,Wales_UnE,NI_UnE)
All_Countries_UnE_Wide <- pivot_wider(All_Countries_UnE,names_from = Country,values_from = Unemployment_Rate)
ggplot(All_Countries_UnE,aes(x=Year,y=Unemployment_Rate,col=Country))+
geom_point()+
ylab("Unemployment Rate (%)") +
xlim(c(2000,2022))
basis_mortality_rates <- create.bspline.basis(rangeval=range(Years),breaks=Years,norder=4)
interior_break_points <- ex_1_data$Time[c(-1,-length(ex_1_data$Time))]
Range_of_Times <- range(ex_1_data$Time)
basis_ex_1 <- create.bspline.basis(Range_of_Times,breaks=interior_break_points,norder=4)
ex_1_smooth_basis <- smooth.basis(ex_1_data$Time,ex_1_data$Y,basis_ex_1)
ex_1_fd_obj <- ex_1_smooth_basis$fd
time_mesh <- seq(0,40,0.01)
function_eval <- data.frame(
Time=time_mesh,
Value=eval.fd(ex_1_fd_obj,time_mesh)
)
ggplot(ex_1_data,aes(x=Time,y=Y))+
geom_point()+
geom_line(data=function_eval,aes(x=Time,y=Value),linetype="dashed")+
geom_line(data=data.frame(Time=Time_Grid,Y=Observations),col="red")
f <- function(vector){
return(sin(1.6*vector))
}
g <- function(vector){
return(sin(vector))
}
f2sq <- function(vector){
sec_d <- 1.6*1.6*sin(1.6*vector)
return(sec_d^2)
}
g2sq <- function(vector){
sec_d <- -1*sin(vector)
return(sec_d^2)
}
plot_df <- data.frame(
X = grid <- seq(0,7,0.02),
f= f(grid),
g= g(grid),
f2 = f2sq(grid),
g2 = g2sq(grid)
) %>%
pivot_longer(c(f,g,f2,g2),names_to = "Function",values_to = "Value") %>%
mutate(Function=as.factor(Function))
levels(plot_df$Function) <- c("f(x)","f''(x)","g(x)","g''(x)")
ggplot(plot_df,aes(x=X,y=Value,col=Function))+
geom_line()+
facet_wrap(~Function) +
ylim(c(-3,7))+
theme(legend.position="none")+
geom_hline(yintercept=0,col="black",linetype="dashed")
observation_matrix <- data.matrix(mortality_rates_wide %>% dplyr::select(-c(Country)))
mortality_rates_wide
observation_matrix <- data.matrix(mortality_rates_wide %>% dplyr::select(-c(Year)))
minimum_lambda <- 0.01
fd_par_obj <- fdPar(basis_mortality_rates,2,minimum_lambda)
sample_of_functions <- smooth.basis(Years,observation_matrix,fd_par_obj)$fd
sample_of_functions$fdnames$time <- "Years"
sample_of_functions$fdnames$values <- "Mortality Rate"
plot(sample_of_functions)
smooth_basis_output <- smooth.basis(Years,observation_matrix,fd_par_obj)
ggplot(residuals_long, aes(sample = nox)) +
stat_qq(distribution = qnorm) +
stat_qq_line(distribution = qnorm) +
facet_wrap(~site)
library(openair)
library(tidyverse)
library(fda)
library(worldmet)
library(openairmaps)
#### Data Importing ####
# Shows All Information About All Monitoring Stations in The AURN network
station_data <- importMeta(source="aurn", all=TRUE)[1:40,]
#gets data for particular site
site_data <- importMeta(source = "aurn") %>%
filter(site == "Dundee Mains Loan")
#site codes for sites in glasgow
glasgow_sites <- c("GLA4","GLKP","GHSR","GGWR")
# imports data for glasgow station for 2022
raw_data <- importAURN(
site=glasgow_sites,
year=2018:2022
)
#### Data Cleaning / Extraction of Relevant Info ####
# stores the names of sites
sites <- unique(raw_data$site)
# filters data for only columns of interest
data_longer <- raw_data %>%
dplyr::select(date,site,nox) %>%
mutate(nox=replace(nox,nox<0,0))
#pivots data so there is a column for each station
data_wider <- pivot_wider(data = data_longer,names_from = "site",values_from = "nox")
#removes rows from data which contain NAs
data_wider_no_NAs <- data_wider %>%
na.omit()
data_longer_no_NAs <- data_wider_no_NAs %>%
pivot_longer(cols=c(2:length(colnames(data_wider_no_NAs))),values_to = 'nox',names_to = 'site')
# creates matrix of observations from wider dataframe with NAs removed
obs_matrix_no_NAs <- as.matrix(data_wider_no_NAs %>% dplyr::select(-c(date)))
# extracts time points being used and scales them so initial observation is taken as 0
time_points <- (as.numeric(data_wider$date)-min(as.numeric(data_wider$date)))*3.80517183071e-7
# repeates process of data with NAs removed
time_points_no_NAs <- (as.numeric(data_wider_no_NAs$date)-min(as.numeric(data_wider_no_NAs$date)))*3.80517183071e-7
#### Plot Summaries of Data ####
# Plots NOx emissions over time for each station
ggplot(data_longer,aes(x=date,y=nox,col=site))+
geom_line()+
facet_wrap(~site)
# Plots NOx emissions over time for each station
ggplot(data_longer_no_NAs,aes(x=date,y=nox,col=site))+
geom_line()+
facet_wrap(~site)
# Plots Wind Speed over time for eaxh station
ggplot(raw_data,aes(x=date,y=ws,col=site))+
geom_line()+
facet_wrap(~site)
#### GCV Function Definition ####
# creates function which outputs the GCV of a functional data object
# fitted using a specific lambda
GCV_func <- function(log_lambda,basis,observations,time_points,penalty){
lambda <- 10^log_lambda
fd_par_obj <- fdPar(basis,penalty,lambda)
smoothbasisobj <- smooth.basis(time_points,observations,fd_par_obj)
return(sum(smoothbasisobj$gcv))
}
#### Functions Fitted Omitting NAs ####
#creates B-Spline basis of order 7 with 60 basis functions
basis_no_NAs <- create.bspline.basis(range(time_points_no_NAs),
nbasis = 60,
norder=7)
# Finds optimal lambda and extracts from output
optimised_function_no_NAs <- optimise(GCV_func,lower=0,upper=10,
basis=basis_no_NAs,observations=obs_matrix_no_NAs,
time_points=time_points_no_NAs,penalty=2)
minimum_log_lambda_no_NAs <- optimised_function_no_NAs$minimum
minimum_lambda_no_NAs <- 10^minimum_log_lambda_no_NAs
# FD Par object created which specifies the type of smoothing as well as the lambda to be used
fd_par_obj_no_NAs <- fdPar(basis_no_NAs,2,minimum_lambda_no_NAs)
# Creates sample of functions
sample_of_functions_no_NAs <- smooth.basis(time_points_no_NAs,obs_matrix_no_NAs,fd_par_obj_no_NAs)
#### Plots of Sample of Functions NAs Removed ####
estimated_data <- as.data.frame(eval.fd(time_points,sample_of_functions_no_NAs$fd)) %>%
mutate(date=data_wider$date) %>%
dplyr::select(c(5,1,2,3,4))
estimated_data_longer <- pivot_longer(estimated_data,names_to = "site",values_to = "nox",cols=c(2,3,4,5))
par(mfrow=c(1,1))
ggplot(estimated_data_longer,aes(x=date,y=nox,col=site))+
geom_line() +
geom_line(data_longer,mapping=aes(x=date,y=nox,col=site),inherit.aes = FALSE,alpha=0.05)
#### Extract Fitted Values and Back-Fill ####
data_NAs_filled <- data_wider
rows_with_NAs <- rowSums(is.na(data_NAs_filled)) > 0
for (i in 1:length(rows_with_NAs)){
if (rows_with_NAs[i] == TRUE){
row_to_be_filled <- data_NAs_filled[i,]
date_to_fill <- row_to_be_filled$date
cols_to_be_filled <- is.na(row_to_be_filled)
sites_to_fill <- colnames(row_to_be_filled)[cols_to_be_filled]
for (j in 1:length(sites_to_fill)){
value <- estimated_data %>%
filter(date==date_to_fill) %>%
dplyr::select(sites_to_fill[j]) %>%
as.numeric()
row_to_be_filled[,colnames(row_to_be_filled) == sites_to_fill[j]] <- value
data_NAs_filled[i,] <- row_to_be_filled
}
}
}
#pivots data so there is a column for each station
data_longer_NAs_filled <- pivot_longer(data = data_NAs_filled,names_to = "site",values_to = "nox",cols=2:length(colnames(data_NAs_filled)))
# creates matrix of observations from wider dataframe
obs_matrix_NAs_filled <- as.matrix(data_NAs_filled %>% dplyr::select(-c(date)))
#### Functions Fitted With NAs replaced ####
# creates basis
basis <- create.bspline.basis(range(time_points),
nbasis = 40,
norder=7)
# Finds optimal lambda and extracts from output
optimised_function <- optimise(GCV_func,lower=0,upper=10,
basis=basis, observations=obs_matrix_NAs_filled,
time_points=time_points,penalty=2)
minimum_log_lambda <- optimised_function$minimum
minimum_lambda <- 10^minimum_log_lambda
# FD Par object created which specifies the type of smoothing as well as the lambda to be used
fd_par_obj <- fdPar(basis,2,minimum_lambda)
sample_of_functions <- smooth.basis(time_points,obs_matrix_NAs_filled,fd_par_obj)
#### Checking Residuals ####
shap_test <- function(vector){
shapiro.test(vector)$p.value
}
residuals_matrix <- eval.fd(time_points,sample_of_functions$fd)-obs_matrix_NAs_filled
residuals_df <- data.frame(residuals_matrix)
colnames(residuals_df) <- colnames(residuals_matrix)
residuals_long <- residuals_df %>%
pivot_longer(values_to = 'nox',names_to = 'site',cols=1:length(colnames(residuals_df)))
ggplot(residuals_long,aes(x=nox))+
geom_histogram()+
facet_wrap(~site)
ggplot(residuals_long, aes(sample = nox)) +
stat_qq(distribution = qnorm) +
stat_qq_line(distribution = qnorm) +
facet_wrap(~site)
MASS::fitdistr(residuals_long$nox, "normal")
MASS::fitdistr(residuals_long$nox, "normal")$estimate
MASS::fitdistr(residuals_long$nox, "normal")$estimate
setwd("~/Dissertation/Dissertation and FDA Content/Datasets for Examples/Open Air/Poverty Index Data")
setwd("~/Dissertation/Dissertation and FDA Content/Datasets for Examples/Open Air/Poverty Index Data")
pov_ind_data <- read.csv("postcode_2020_1_simd2020v2.csv")
head(pov_ind_data)
raw_data$site
importMeta
importMeta()
library(openair)
library(tidyverse)
library(fda)
library(worldmet)
library(openairmaps)
importMeta()
importMeta(all=TRUE)
devtools::install_github("ropensci/PostcodesioR")
library(PostcodesioR)
reverse_geocoding(0.127, 51.507)
reverse_geocoding(0.127, 51.507)[[1]]
reverse_geocoding(0.127, 51.507)[[1]]$postcode
#gets data for particular site
site_data <- importMeta(source = "aurn") %>%
filter(site %in% sites)
site_data
reverse_geocoding(site_data$latitude,site_data$longitude)
get_post_func <- function(longitude,latitude){
postcodes <- c()
for (i in 1:length(longitude)){
postcodes <- c(postcodes,reverse_geocoding(longitude=longitude[i],latitude=latitude[i]))[[1]]$postcode)
get_post_func <- function(longitude,latitude){
postcodes <- c()
for (i in 1:length(longitude)){
postcodes <- c(postcodes,reverse_geocoding(longitude=longitude[i],latitude=latitude[i])[[1]]$postcode)
}
return(postcodes)
}
get_post_func(site_data$latitude,site_data$longitude)
reverse_geocoding(latitude=site_data$latitude[1],longitude=site_data$longitude[1])
reverse_geocoding(latitude=site_data$latitude[1],longitude=site_data$longitude[1])[[1]]
reverse_geocoding(latitude=site_data$latitude[1],longitude=site_data$longitude[1])[[1]]$postcode
get_post_func <- function(longitude,latitude){
postcodes <- c()
for (i in 1:length(longitude)){
postcodes <- c(postcodes,reverse_geocoding(longitude=longitude[i],latitude=latitude[i])[[1]]$postcode)
}
return(postcodes)
}
get_post_func(site_data$latitude[1],site_data$longitude[1])
get_post_func <- function(longitude,latitude){
postcodes <- c()
for (i in 1:length(longitude)){
postcodes <- c(postcodes,reverse_geocoding(longitude=longitude[i],latitude=latitude[i])[[1]]$postcode)
}
return(postcodes)
}
get_post_func(longitude=site_data$latitude[1],latitude=site_data$longitude[1])
get_post_func(latitude=site_data$latitude[1],longitude=site_data$longitude[1])
get_post_func(latitude=site_data$latitude,longitude=site_data$longitude)
setwd("~/Dissertation/Dissertation and FDA Content/Datasets for Examples/Open Air/Poverty Index Data")
pov_ind_data <- read.csv("postcode_2020_1_simd2020v2.csv")
head(pov_ind_data)
get_post_func <- function(longitude,latitude){
postcodes <- c()
for (i in 1:length(longitude)){
postcodes <- c(postcodes,reverse_geocoding(longitude=longitude[i],latitude=latitude[i])[[1]]$postcode)
}
return(postcodes)
}
#gets data for particular site
site_data <- importMeta(source = "aurn") %>%
filter(site %in% sites) %>%
mutate(Postcode = get_post_func(longitude,latitude))
site_data
pov_ind_data %>%
filter(pc7 == site_data$Postcode[1])
pov_ind_data %>%
filter(pc7 == site_data$Postcode[2])
pov_ind_data %>%
filter(pc7 == site_data$Postcode[2])
head(pov_ind_data)
unique(pov_ind_data$pc8)
get_post_func <- function(longitude,latitude){
postcodes <- c()
for (i in 1:length(longitude)){
postcodes <- c(postcodes,reverse_geocoding(longitude=longitude[i],latitude=latitude[i])[[1]]$postcode)
}
return(postcodes)
}
#gets data for particular site
site_data <- importMeta(source = "aurn") %>%
filter(site %in% sites) %>%
mutate(Postcode = get_post_func(longitude,latitude))
pov_ind_data %>%
filter(pc7 == site_data$Postcode[2])
postcodes <- pov_ind_data$pc7
postcodes2 <- site_data$Postcode
postcodes %in% postcodes2 == TRUE
postcodes[postcodes %in% postcodes2]
starts_with(postcodes,"G")
startsWith(unique(postcodes),"G")
postcodes[startsWith(unique(postcodes),"G")]
postcodes2
postcodes[startsWith(unique(postcodes),"G4")]
pov_ind_data <- read.csv("postcode_2020_1_simd2020v2.csv") %>%
mutate(pc7=gsub("  "," ",pc7))
postcodes[postcodes %in% postcodes2]
postcodes <- pov_ind_data$pc7
postcodes[postcodes %in% postcodes2]
pov_ind_data %>%
filter(pc7 %in% site_data$Postcode)
pov_ind_Data_for_stations <- pov_ind_data %>%
filter(pc7 %in% site_data$Postcode)
pov_ind_data_for_stations <- pov_ind_data %>%
filter(pc7 %in% site_data$Postcode)
pov_ind_data
pov_ind_data_for_stations <- pov_ind_data %>%
filter(pc7 %in% site_data$Postcode) %>%
dplyr::select(c(pc7,simd2020v2_sc_decile))
full_join(site_data,pov_ind_data)
full_join(site_data,pov_ind_data,by=Postcode)
full_join(site_data,pov_ind_data,by="Postcode")
site_data
pov_ind_data_for_stations <- pov_ind_data %>%
filter(pc7 %in% site_data$Postcode) %>%
rename("Postcode"=pc7) %>%
dplyr::select(c(Postcode,simd2020v2_sc_decile))
full_join(site_data,pov_ind_data_for_stations,by="Postcode")
site_data_full <- full_join(site_data,pov_ind_data_for_stations,by="Postcode")
